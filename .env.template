# Tipo de aplicação a executar
# Valores válidos: PRODUTOR_AVRO, CONSUMIDOR_AVRO, PRODUTOR_JSON, CONSUMIDOR_JSON
TIPO_APLICACAO=PRODUTOR_AVRO

# Total de mensagens a processar
TOTAL_MENSAGENS=10000

# Tamanho aproximado das mensagens em KB (1024 = ~1MB, 2048 = ~2MB)
TAMANHO_MENSAGEM_KB=25

# Número de partições do tópico (usar 18 conforme novo setup)
NUM_PARTICOES=18

# Número de threads de consumidor por processo (opcional; padrão = NUM_PARTICOES)
CONSUMER_THREADS=

# Modo de benchmark: TRANSPORTE (não parseia), E2E_PARSE (parse JSON/Avro), MICRO (sem Kafka)
BENCH_MODE=E2E_PARSE

# Compressão para o produtor: none|lz4|zstd|gzip (padrão lz4)
COMPRESSION_TYPE=none

# Registrar schema automaticamente (true/false)
AUTO_REGISTER_SCHEMAS=true

# Quantidade de mensagens de warmup (não contadas nas métricas principais)
WARMUP_MENSAGENS=0

# Configurações do Confluent Cloud Kafka Cluster
bootstrap_server=
kafka_cluster_api_key=
kafka_cluster_api_secret=

# Configurações do Confluent Cloud Schema Registry
schema_registry_rest_endpoint=
schema_registry_api_key=
schema_registry_api_secret=